{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube Comment Spam Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEq+w4WsgGy7aeFOUbOBzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharthapdutta/Machine-Learning-Projects/blob/master/YouTube_Comment_Spam_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjjDSTPoVkS_"
      },
      "source": [
        "# YouTube Comment Spam Classification\n",
        "Made on [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwnMJXJvVwGg"
      },
      "source": [
        "## Obtaining the Dataset\n",
        "[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uggU7GNBVd9V",
        "outputId": "fe71931f-70d5-48db-ce3f-4b5bf1977b95"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
        "!unzip YouTube-Spam-Collection-v1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-15 10:09:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163567 (160K) [application/x-httpd-php]\n",
            "Saving to: ‘YouTube-Spam-Collection-v1.zip’\n",
            "\n",
            "\r          YouTube-S   0%[                    ]       0  --.-KB/s               \rYouTube-Spam-Collec 100%[===================>] 159.73K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-08-15 10:09:50 (5.40 MB/s) - ‘YouTube-Spam-Collection-v1.zip’ saved [163567/163567]\n",
            "\n",
            "Archive:  YouTube-Spam-Collection-v1.zip\n",
            "  inflating: Youtube01-Psy.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Youtube01-Psy.csv  \n",
            "  inflating: Youtube02-KatyPerry.csv  \n",
            "  inflating: __MACOSX/._Youtube02-KatyPerry.csv  \n",
            "  inflating: Youtube03-LMFAO.csv     \n",
            "  inflating: __MACOSX/._Youtube03-LMFAO.csv  \n",
            "  inflating: Youtube04-Eminem.csv    \n",
            "  inflating: __MACOSX/._Youtube04-Eminem.csv  \n",
            "  inflating: Youtube05-Shakira.csv   \n",
            "  inflating: __MACOSX/._Youtube05-Shakira.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrl6oQ63V-DO"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzbRwZlfWWxj"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pickle import dump, load\n",
        "from os import environ\n",
        "from googleapiclient.discovery import build  # YouTube API\n",
        "from urllib.parse import urlparse, parse_qs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp-z1w4aWxUW"
      },
      "source": [
        "## Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "ym50bgbCWZRN",
        "outputId": "118231b1-540b-4ad8-98e4-02bc4b959408"
      },
      "source": [
        "# Loading Datasets\n",
        "df = pd.concat([pd.read_csv('Youtube01-Psy.csv'),\n",
        "                pd.read_csv('Youtube02-KatyPerry.csv'),\n",
        "                pd.read_csv('Youtube03-LMFAO.csv'),\n",
        "                pd.read_csv('Youtube04-Eminem.csv'),\n",
        "                pd.read_csv('Youtube05-Shakira.csv')])\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COMMENT_ID</th>\n",
              "      <th>AUTHOR</th>\n",
              "      <th>DATE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
              "      <td>Katie Mettam</td>\n",
              "      <td>2013-07-13T13:27:39.441000</td>\n",
              "      <td>I love this song because we sing it at Camp al...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
              "      <td>Sabina Pearson-Smith</td>\n",
              "      <td>2013-07-13T13:14:30.021000</td>\n",
              "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
              "      <td>jeffrey jules</td>\n",
              "      <td>2013-07-13T12:09:31.188000</td>\n",
              "      <td>wow</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
              "      <td>Aishlin Maciel</td>\n",
              "      <td>2013-07-13T11:17:52.308000</td>\n",
              "      <td>Shakira u are so wiredo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
              "      <td>Latin Bosch</td>\n",
              "      <td>2013-07-12T22:33:27.916000</td>\n",
              "      <td>Shakira is the best dancer</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      COMMENT_ID  ... CLASS\n",
              "365  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA  ...     0\n",
              "366  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  ...     0\n",
              "367  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs  ...     0\n",
              "368  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0  ...     0\n",
              "369  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjm7TpOjWiev",
        "outputId": "a4e1c788-3c51-4679-b12d-bbc18019a7ab"
      },
      "source": [
        "# Filtering Dataset\n",
        "df = df[['CONTENT', 'CLASS']]  # Only require the comment content and its class\n",
        "print('Spam Comments:', sum(df.CLASS == 1))  # Spam is classified as 1\n",
        "print('Non-Spam Comments:', sum(df.CLASS == 0))  # Not spam is classified as 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam Comments: 1005\n",
            "Non-Spam Comments: 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUjqdTLmjsX4",
        "outputId": "805a7b33-c48d-473e-83c9-7f7de98712e6"
      },
      "source": [
        "# Performing Train-Test Split\n",
        "x = df['CONTENT']\n",
        "y = df['CLASS']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
        "print(f'Training Data - x_train: {x_train.shape}  y_train: {y_train.shape}')\n",
        "print(f'Testing Data  - x_test:  {x_test.shape}   y_test: {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data - x_train: (1467,)  y_train: (1467,)\n",
            "Testing Data  - x_test:  (489,)   y_test: (489,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3JOZSxoiiWH"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMGFqvA7ih2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb28adf-63e1-49cf-eeeb-a462a929f1d2"
      },
      "source": [
        "# Setting up a pipeline\n",
        "pipeline = make_pipeline(CountVectorizer(),\n",
        "                         TfidfTransformer(norm=None), \n",
        "                         RandomForestClassifier())\n",
        "pipeline.steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('countvectorizer',\n",
              "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                  tokenizer=None, vocabulary=None)),\n",
              " ('tfidftransformer',\n",
              "  TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
              " ('randomforestclassifier',\n",
              "  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                         criterion='gini', max_depth=None, max_features='auto',\n",
              "                         max_leaf_nodes=None, max_samples=None,\n",
              "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                         min_samples_leaf=1, min_samples_split=2,\n",
              "                         min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                         n_jobs=None, oob_score=False, random_state=None,\n",
              "                         verbose=0, warm_start=False))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrSuZsf_kmyI",
        "outputId": "f682de33-a9d9-476e-ad7a-e6fe30d365f2"
      },
      "source": [
        "pipeline.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('countvectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, voc...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voTIASDwlF9E"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWD_GfYglesC",
        "outputId": "05300f9e-2ba0-4be7-b561-a82307d62219"
      },
      "source": [
        "# Evaulate against testing data\n",
        "accuracy = pipeline.score(x_test, y_test)\n",
        "print(\"Accuracy: %0.3f\" % (accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1tKco9tmC-o",
        "outputId": "58442931-e845-429d-ebd4-ab120c99651a"
      },
      "source": [
        "# Cross validation evaluation\n",
        "scores = cross_val_score(pipeline, x, y, cv=5)\n",
        "print(\"Accuracy %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()*2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.94 (+/- 0.03)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfg6st-acdE1",
        "outputId": "0dff4be2-e5ee-490a-b35c-480c08e6cbde"
      },
      "source": [
        "# Confusion matrix\n",
        "y_pred = pipeline.predict(x_test)\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[234,   3],\n",
              "       [ 14, 238]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYgOvIsFm8RT"
      },
      "source": [
        "## Model Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB_Y31n5m-yX",
        "outputId": "f29362aa-9651-4a62-a1ae-9a2eb98af199"
      },
      "source": [
        "# Best Parameters Search using Grid Search\n",
        "parameters = {\n",
        "    'countvectorizer__max_features': (None, 1000, 2000),  # Number of words\n",
        "    'countvectorizer__ngram_range': ((1, 1), (1, 2)),  # Unigrams or bigrams\n",
        "    'countvectorizer__stop_words': ('english', None),  # Stop words inclusion\n",
        "    'tfidftransformer__use_idf': (True, False),  # TF-IDF use\n",
        "    'randomforestclassifier__n_estimators': (20, 50, 100)  # Classifier estimators\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=pipeline, \n",
        "                           param_grid=parameters,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=1)\n",
        "grid_search.fit(x, y)  # Use data for grid search"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   10.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   49.5s\n",
            "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('countvectorizer',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token...\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
              "                         'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
              "                         'countvectorizer__stop_words': ('english', None),\n",
              "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
              "                         'tfidftransformer__use_idf': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je79hnNhp-cc",
        "outputId": "7ae16828-3312-458f-9b0e-80a5c15f4c94"
      },
      "source": [
        "# Use Data for Grid Search\n",
        "grid_search.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    9.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   47.1s\n",
            "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('countvectorizer',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token...\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
              "                         'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
              "                         'countvectorizer__stop_words': ('english', None),\n",
              "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
              "                         'tfidftransformer__use_idf': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT0VPX4TqHnp",
        "outputId": "d4b07227-69d8-4074-d16d-cbdbef47e4df"
      },
      "source": [
        "# Evaluation of Best Parameters\n",
        "print(\"Best Accuracy: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best Parameter Set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for parameter in sorted(parameters.keys()):\n",
        "  print(\"\\t%s:\\t%r\" % (parameter, best_parameters[parameter]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy: 0.944\n",
            "Best Parameter Set:\n",
            "\tcountvectorizer__max_features:\tNone\n",
            "\tcountvectorizer__ngram_range:\t(1, 2)\n",
            "\tcountvectorizer__stop_words:\tNone\n",
            "\trandomforestclassifier__n_estimators:\t100\n",
            "\ttfidftransformer__use_idf:\tFalse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTgEZj3artmu"
      },
      "source": [
        "# Save Best Model\n",
        "dump(grid_search.best_estimator_, open(\"YouTube_Spam_Classifier.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8owGyQttLj"
      },
      "source": [
        "## Using the ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_of3GAOZMmAf"
      },
      "source": [
        "Uses the [YouTube Data API](https://developers.google.com/youtube/v3) to obtain top-level comments from a YouTube video. \n",
        "\n",
        "[YouTube API Credentials](https://www.geeksforgeeks.org/youtube-data-api-set-1/) are required.\n",
        "\n",
        "Set environment variable using the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq6oMwicIGl8"
      },
      "source": [
        "%env API_KEY=<YOUR_API_KEY_HERE>\n",
        "DEVELOPER_KEY = environ.get('API_KEY')\n",
        "try:\n",
        "  assert DEVELOPER_KEY\n",
        "except AssertionError:\n",
        "  raise Exception(\"API_KEY is required.\")\n",
        "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "YOUTUBE_API_VERSION = \"v3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-n-VYcSIs2n"
      },
      "source": [
        "def get_video_comments(video_id):\n",
        "  ''' Returns top level commments for a youtube video. '''\n",
        "  youtube = build(YOUTUBE_API_SERVICE_NAME,\n",
        "                  YOUTUBE_API_VERSION,\n",
        "                  developerKey=DEVELOPER_KEY)\n",
        "  video_response = youtube.commentThreads().list(\n",
        "      part='snippet',\n",
        "      videoId=video_id\n",
        "  ).execute()\n",
        "\n",
        "  comments = list()\n",
        "  while video_response:\n",
        "    for item in video_response['items']:\n",
        "      comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "      comments.append(comment)\n",
        "    \n",
        "    if 'nextPageToken' in video_response:\n",
        "      video_response = youtube.commentThreads().list(\n",
        "          part='snippet',\n",
        "          videoId=video_id,\n",
        "          pageToken=video_response['nextPageToken']\n",
        "      ).execute()\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35n_yXOoItNW",
        "outputId": "0ec2eb51-b571-41c8-e064-bc49e6210b1c"
      },
      "source": [
        "# Retrieve comments from YouTube URL\n",
        "youtube_link = urlparse(input(\"Enter YouTube URL: \"))\n",
        "youtube_id = parse_qs(youtube_link.query)['v'][0]\n",
        "comments = get_video_comments(youtube_id)\n",
        "print(f'{len(comments)} top-level comments found.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter YouTube URL: https://www.youtube.com/watch?v=Vhh_GeBPOhs\n",
            "1710 top-level comments found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2rgdg5Qspdg"
      },
      "source": [
        "# Use trained model to classify comments\n",
        "model = load(open('YouTube_Spam_Classifier.pkl','rb'))\n",
        "pred = model.predict(comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgU8fVYmtx36",
        "outputId": "1bd95150-f614-4bfc-ef19-0d6df7391d5a"
      },
      "source": [
        "# Parse predictions\n",
        "results = {\n",
        "    'spam': {\n",
        "        'count': 0,\n",
        "        'comments': list()\n",
        "    },\n",
        "    'non-spam': {\n",
        "        'count': 0,\n",
        "        'comments': list()\n",
        "    }\n",
        "}\n",
        "for comment, clazz in zip(comments, pred):\n",
        "  if clazz == 1:\n",
        "    results['spam']['count'] += 1\n",
        "    results['spam']['comments'].append(comment)\n",
        "  else:\n",
        "    results['non-spam']['count'] += 1\n",
        "    results['non-spam']['comments'].append(comment)\n",
        "\n",
        "\n",
        "print('Spam comments found:', results['spam']['count'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam comments found: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9QQB9JTRb8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}